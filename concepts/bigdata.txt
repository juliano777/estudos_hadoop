O que é o Hadoop?

    Apache Hadoop é um framework open source para armazenamento e processamento em larga escala de conjuntos de dados em clusters.
    A ideia essencial do Hadoop é em vez de mover os dados para o processamento, move o processamento para os dados.

Componentes Básicos do Hadoop

- Hadoop Common: Contém bibliotecas e utilitários necessários para outros módulos do Hadoop;
- Hadoop Distributed File System (HDFS): É um sistema de arquivos escalável distribuído, feito para o framework Hadoop;
- Hadoop YARN: É um gerenciador de recursos para gerenciar recursos computacionais e utilizá-los para agendar usuários e aplicações;
- Hadoop MapReduce: É um modelo de programação que escala dados através de vários diferentes processos.

Grandes Componentes do Ecossistema Hadoop

- Sqoop: Ferramenta de linha de comando para ingestão de dados entre o Hadoop e bases de dados relacionais. Seu nome significa "SQL for Hadoop";
- HBase: Banco de dados NoSQL orientado a colunas, armazenamento chave-valor. 
- Pig: Programação de alto nível sobre o MapReduce. Utiliza a linguagem chamada Pig Latin. Resolve questões de análise de dados em fluxos de dados.
- Hive: É um software de data warehouse que interage com os dados residentes no armazenamento distribuído (e. g. HDFS, S3A, etc). A interação se dá com a linguagem HQL (Hive Query Language ou Hive QL), que é muito similar à linguagem SQL;
- Oozie: Sistema agendador de workflow para jobs do Hadoop. Suporta MapReduce, Pig, Hive, Sqoop, Spark e etc;
- Zookeeper: Centralized service for maintaining , configuration information, naming, providing distributed synchronization and providing group services;
- Flume: Serviço distribuído para eficientemente coletar, agregar e mover grande quantidade de dados de logs, cuja arquitetura é muito simples baseada em streaming e fluxos de dados;
- Impala: É um software para MPP (Massively Parallel Processing), que utiliza engine SQL para dados armazenados no cluster Hadoop;
- Spark: É um engine geral e muito rápido para processamento dados em larga escala, pois trabalha "in memory". Permite a aplicações de usuário carregar dados na memória do cluster e consultá-los repetidamente. Pode rodar sobre o Yarn. Pode interagir com diferentes tipos de armazenamentos como HDFS e Amazon S3. Adequado para machine learning;
